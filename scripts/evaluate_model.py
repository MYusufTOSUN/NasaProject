#!/usr/bin/env python3
"""
Metrik deÄŸerlendirme scripti - confusion matrix, ROC curve, performans raporlarÄ±
predictions_detail.csv veya all_graphs_predictions.csv dosyasÄ±ndan metrikleri hesaplar.
"""

import argparse
import sys
from pathlib import Path
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_curve, auc,
    precision_recall_curve, average_precision_score
)
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')


def load_predictions(pred_csv):
    """
    Tahmin sonuÃ§larÄ±nÄ± yÃ¼kle.
    
    Args:
        pred_csv: Predictions CSV dosyasÄ±
        
    Returns:
        DataFrame veya None
    """
    print(f"âœ“ Tahminler yÃ¼kleniyor: {pred_csv}")
    
    pred_path = Path(pred_csv)
    if not pred_path.exists():
        print(f"HATA: Dosya bulunamadÄ±: {pred_csv}")
        return None
    
    try:
        df = pd.read_csv(pred_csv)
        print(f"  {len(df)} satÄ±r yÃ¼klendi")
        
        # Gerekli kolonlarÄ± kontrol et
        required_cols = ['image_path', 'pred_label', 'prob_positive', 'prob_negative']
        missing_cols = [col for col in required_cols if col not in df.columns]
        
        if missing_cols:
            print(f"HATA: Eksik kolonlar: {missing_cols}")
            return None
        
        return df
        
    except Exception as e:
        print(f"HATA: Dosya okuma hatasÄ±: {e}")
        return None


def check_true_labels(df):
    """
    True label kolonunu kontrol et ve hazÄ±rla.
    
    Args:
        df: DataFrame
        
    Returns:
        tuple: (has_labels, df)
    """
    print("\nâœ“ True label kontrolÃ¼...")
    
    # true_label kolonu var mÄ±?
    if 'true_label' in df.columns:
        # BoÅŸ deÄŸerler var mÄ± kontrol et
        non_empty = df['true_label'].notna() & (df['true_label'] != '')
        
        if non_empty.sum() > 0:
            # String'leri int'e Ã§evir
            df.loc[non_empty, 'true_label'] = df.loc[non_empty, 'true_label'].astype(int)
            
            labeled_count = non_empty.sum()
            print(f"  {labeled_count}/{len(df)} gÃ¶rsel iÃ§in true label mevcut")
            
            # DaÄŸÄ±lÄ±mÄ± gÃ¶ster
            label_dist = df.loc[non_empty, 'true_label'].value_counts().to_dict()
            print(f"  DaÄŸÄ±lÄ±m: {label_dist}")
            
            return True, df
        else:
            print("  âš  true_label kolonu var ama hepsi boÅŸ")
            return False, df
    else:
        print("  â„¹ true_label kolonu yok (sadece tahminler)")
        return False, df


def calculate_metrics(y_true, y_pred, y_proba_pos):
    """
    Temel metrikleri hesapla.
    
    Args:
        y_true: GerÃ§ek etiketler
        y_pred: Tahmin edilen etiketler
        y_proba_pos: Positive sÄ±nÄ±f olasÄ±lÄ±klarÄ±
        
    Returns:
        dict: Metrikler
    """
    print("\nâœ“ Metrikler hesaplanÄ±yor...")
    
    # Temel metrikler
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='binary', zero_division=0)
    recall = recall_score(y_true, y_pred, average='binary', zero_division=0)
    f1 = f1_score(y_true, y_pred, average='binary', zero_division=0)
    
    metrics = {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1
    }
    
    # ROC-AUC
    try:
        fpr, tpr, _ = roc_curve(y_true, y_proba_pos)
        roc_auc = auc(fpr, tpr)
        metrics['roc_auc'] = roc_auc
        
        # PR-AUC
        precision_vals, recall_vals, _ = precision_recall_curve(y_true, y_proba_pos)
        pr_auc = average_precision_score(y_true, y_proba_pos)
        metrics['pr_auc'] = pr_auc
        
    except Exception as e:
        print(f"  âš  AUC metrikleri hesaplanamadÄ±: {e}")
        metrics['roc_auc'] = 0.0
        metrics['pr_auc'] = 0.0
    
    print(f"  Accuracy: {accuracy:.4f}")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall: {recall:.4f}")
    print(f"  F1-Score: {f1:.4f}")
    if 'roc_auc' in metrics:
        print(f"  ROC-AUC: {metrics['roc_auc']:.4f}")
    
    return metrics


def create_confusion_matrix(y_true, y_pred, output_dir):
    """
    Confusion matrix oluÅŸtur ve kaydet.
    
    Args:
        y_true: GerÃ§ek etiketler
        y_pred: Tahmin edilen etiketler
        output_dir: Ã‡Ä±ktÄ± klasÃ¶rÃ¼
        
    Returns:
        ndarray: Confusion matrix
    """
    print("\nâœ“ Confusion matrix oluÅŸturuluyor...")
    
    cm = confusion_matrix(y_true, y_pred)
    
    # Grafik oluÅŸtur
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['Negative (0)', 'Positive (1)'],
                yticklabels=['Negative (0)', 'Positive (1)'],
                cbar_kws={'label': 'Count'})
    
    plt.title('Confusion Matrix\nExoplanet Transit Detection', fontsize=14, fontweight='bold')
    plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')
    plt.ylabel('True Label', fontsize=12, fontweight='bold')
    
    # YÃ¼zdeleri ekle
    cm_sum = cm.sum()
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            percentage = cm[i, j] / cm_sum * 100
            plt.text(j + 0.5, i + 0.7, f'({percentage:.1f}%)', 
                    ha='center', va='center', fontsize=10, color='gray')
    
    # Kaydet
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    cm_path = output_path / 'confusion_matrix.png'
    plt.savefig(cm_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"  Kaydedildi: {cm_path}")
    
    return cm


def create_roc_curve(y_true, y_proba_pos, output_dir):
    """
    ROC curve oluÅŸtur ve kaydet.
    
    Args:
        y_true: GerÃ§ek etiketler
        y_proba_pos: Positive sÄ±nÄ±f olasÄ±lÄ±klarÄ±
        output_dir: Ã‡Ä±ktÄ± klasÃ¶rÃ¼
        
    Returns:
        float: ROC-AUC deÄŸeri
    """
    print("\nâœ“ ROC curve oluÅŸturuluyor...")
    
    try:
        # ROC curve hesapla
        fpr, tpr, thresholds = roc_curve(y_true, y_proba_pos)
        roc_auc = auc(fpr, tpr)
        
        # Grafik
        plt.figure(figsize=(10, 8))
        plt.plot(fpr, tpr, color='darkorange', lw=2, 
                label=f'ROC curve (AUC = {roc_auc:.4f})')
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', 
                label='Random Classifier (AUC = 0.5)')
        
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')
        plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')
        plt.title('ROC Curve - Exoplanet Transit Detection', fontsize=14, fontweight='bold')
        plt.legend(loc="lower right", fontsize=10)
        plt.grid(True, alpha=0.3)
        
        # Kaydet
        output_path = Path(output_dir)
        roc_path = output_path / 'roc_curve.png'
        plt.savefig(roc_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  Kaydedildi: {roc_path}")
        print(f"  ROC-AUC: {roc_auc:.4f}")
        
        return roc_auc
        
    except Exception as e:
        print(f"  âš  ROC curve oluÅŸturulamadÄ±: {e}")
        return None


def create_pr_curve(y_true, y_proba_pos, output_dir):
    """
    Precision-Recall curve oluÅŸtur ve kaydet.
    
    Args:
        y_true: GerÃ§ek etiketler
        y_proba_pos: Positive sÄ±nÄ±f olasÄ±lÄ±klarÄ±
        output_dir: Ã‡Ä±ktÄ± klasÃ¶rÃ¼
        
    Returns:
        float: PR-AUC deÄŸeri
    """
    print("\nâœ“ Precision-Recall curve oluÅŸturuluyor...")
    
    try:
        # PR curve hesapla
        precision_vals, recall_vals, _ = precision_recall_curve(y_true, y_proba_pos)
        pr_auc = average_precision_score(y_true, y_proba_pos)
        
        # Grafik
        plt.figure(figsize=(10, 8))
        plt.plot(recall_vals, precision_vals, color='blue', lw=2, 
                label=f'PR curve (AUC = {pr_auc:.4f})')
        
        # Baseline (pozitif sÄ±nÄ±f oranÄ±)
        baseline = sum(y_true) / len(y_true)
        plt.axhline(y=baseline, color='red', linestyle='--', lw=2,
                   label=f'Baseline (ratio = {baseline:.4f})')
        
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('Recall', fontsize=12, fontweight='bold')
        plt.ylabel('Precision', fontsize=12, fontweight='bold')
        plt.title('Precision-Recall Curve - Exoplanet Transit Detection', fontsize=14, fontweight='bold')
        plt.legend(loc="lower left", fontsize=10)
        plt.grid(True, alpha=0.3)
        
        # Kaydet
        output_path = Path(output_dir)
        pr_path = output_path / 'precision_recall_curve.png'
        plt.savefig(pr_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  Kaydedildi: {pr_path}")
        print(f"  PR-AUC: {pr_auc:.4f}")
        
        return pr_auc
        
    except Exception as e:
        print(f"  âš  PR curve oluÅŸturulamadÄ±: {e}")
        return None


def analyze_errors(df_labeled, output_dir):
    """
    HatalÄ± tahminleri analiz et.
    
    Args:
        df_labeled: Labeled DataFrame
        output_dir: Ã‡Ä±ktÄ± klasÃ¶rÃ¼
        
    Returns:
        DataFrame: Hata analizi
    """
    print("\nâœ“ Hata analizi yapÄ±lÄ±yor...")
    
    # HatalarÄ± bul
    df_labeled['correct'] = df_labeled['true_label'] == df_labeled['pred_label']
    errors_df = df_labeled[~df_labeled['correct']].copy()
    
    if len(errors_df) == 0:
        print("  â„¹ HiÃ§ hata yok!")
        return pd.DataFrame()
    
    print(f"  {len(errors_df)} hatalÄ± tahmin bulundu")
    
    # Hata tipleri
    errors_df['error_type'] = errors_df.apply(
        lambda row: 'False Positive' if row['true_label'] == 0 else 'False Negative',
        axis=1
    )
    
    # GÃ¼ven skoru
    errors_df['confidence'] = errors_df.apply(
        lambda row: max(row['prob_positive'], row['prob_negative']),
        axis=1
    )
    
    # SÄ±rala
    errors_df = errors_df.sort_values('confidence', ascending=False)
    
    # Ä°statistikler
    error_types = errors_df['error_type'].value_counts()
    print(f"  False Positive: {error_types.get('False Positive', 0)}")
    print(f"  False Negative: {error_types.get('False Negative', 0)}")
    
    # Kaydet
    output_path = Path(output_dir)
    errors_path = output_path / 'error_analysis.csv'
    errors_df[['image_path', 'true_label', 'pred_label', 'prob_positive', 'prob_negative', 
               'confidence', 'error_type']].to_csv(errors_path, index=False)
    
    print(f"  Kaydedildi: {errors_path}")
    
    return errors_df


def save_metrics_summary(metrics, cm, output_dir):
    """
    Metrik Ã¶zetini CSV'ye kaydet.
    
    Args:
        metrics: Metrik dictionary
        cm: Confusion matrix
        output_dir: Ã‡Ä±ktÄ± klasÃ¶rÃ¼
    """
    print("\nâœ“ Metrik Ã¶zeti kaydediliyor...")
    
    summary = {
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'accuracy': metrics['accuracy'],
        'precision': metrics['precision'],
        'recall': metrics['recall'],
        'f1_score': metrics['f1_score'],
        'roc_auc': metrics.get('roc_auc', 0.0),
        'pr_auc': metrics.get('pr_auc', 0.0),
        'true_negative': int(cm[0, 0]) if cm is not None else 0,
        'false_positive': int(cm[0, 1]) if cm is not None else 0,
        'false_negative': int(cm[1, 0]) if cm is not None else 0,
        'true_positive': int(cm[1, 1]) if cm is not None else 0
    }
    
    output_path = Path(output_dir)
    summary_path = output_path / 'summary.csv'
    
    pd.DataFrame([summary]).to_csv(summary_path, index=False)
    
    print(f"  Kaydedildi: {summary_path}")


def load_split_summary():
    """
    Dataset split Ã¶zetini yÃ¼kle (varsa).
    
    Returns:
        dict veya None
    """
    split_summary_path = Path('evaluation_results/split_summary.csv')
    
    if not split_summary_path.exists():
        return None
    
    try:
        df = pd.read_csv(split_summary_path)
        
        summary = {}
        for _, row in df.iterrows():
            split = row['split']
            summary[split] = {
                'positive': int(row['positive']),
                'negative': int(row['negative']),
                'total': int(row['total']),
                'num_targets': int(row['num_targets'])
            }
        
        return summary
        
    except Exception as e:
        print(f"  âš  Split summary okunamadÄ±: {e}")
        return None


def update_performance_report(metrics, cm, errors_df, output_dir):
    """
    MODEL_PERFORMANCE_REPORT.md dosyasÄ±nÄ± gÃ¼ncelle.
    
    Args:
        metrics: Metrikler
        cm: Confusion matrix
        errors_df: Hata DataFrame
        output_dir: Ã‡Ä±ktÄ± klasÃ¶rÃ¼
    """
    print("\nâœ“ Performans raporu gÃ¼ncelleniyor...")
    
    # Rapor iÃ§eriÄŸini hazÄ±rla
    current_date = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    # Dataset split bilgilerini yÃ¼kle
    split_summary = load_split_summary()
    
    # YanlÄ±ÅŸ sÄ±nÄ±flanan Ã¶rnekler (en fazla 15)
    error_list_str = ""
    if len(errors_df) > 0:
        top_errors = errors_df.head(15)
        for idx, row in top_errors.iterrows():
            filename = Path(row['image_path']).name
            error_list_str += f"- **{filename}**\n"
            error_list_str += f"  - True: {row['true_label']} | Pred: {row['pred_label']} | "
            error_list_str += f"Confidence: {row['confidence']:.4f} | Type: {row['error_type']}\n"
    else:
        error_list_str = "- HiÃ§ hata yok! MÃ¼kemmel performans! ğŸ‰\n"
    
    # Rapor iÃ§eriÄŸi
    report_content = f"""# Model Performance Report

## 1. Overview
**Version**: 1.0  
**Date**: {current_date}  
**Dataset**: NASA Exoplanet Transit Detection

## 2. Dataset Split
"""
    
    if split_summary:
        for split_name in ['train', 'val', 'test']:
            if split_name in split_summary:
                split_data = split_summary[split_name]
                report_content += f"""
**{split_name.capitalize()} Set**:
- Positive: {split_data['positive']} samples
- Negative: {split_data['negative']} samples
- Total: {split_data['total']} samples
- Unique targets: {split_data['num_targets']}
"""
    else:
        report_content += """
**Training Set**: 
- Positive: [Veri yok] samples
- Negative: [Veri yok] samples
- Total: [Veri yok] samples

**Validation Set**:
- Positive: [Veri yok] samples  
- Negative: [Veri yok] samples
- Total: [Veri yok] samples

**Test Set**:
- Positive: [Veri yok] samples
- Negative: [Veri yok] samples  
- Total: [Veri yok] samples
"""
    
    report_content += f"""
## 3. Classification Metrics

| Metric | Value | Percentage |
|--------|-------|------------|
| **Accuracy** | {metrics['accuracy']:.4f} | {metrics['accuracy']*100:.2f}% |
| **Precision** | {metrics['precision']:.4f} | {metrics['precision']*100:.2f}% |
| **Recall** | {metrics['recall']:.4f} | {metrics['recall']*100:.2f}% |
| **F1-Score** | {metrics['f1_score']:.4f} | {metrics['f1_score']*100:.2f}% |
| **ROC-AUC** | {metrics.get('roc_auc', 0.0):.4f} | {metrics.get('roc_auc', 0.0)*100:.2f}% |
| **PR-AUC** | {metrics.get('pr_auc', 0.0):.4f} | {metrics.get('pr_auc', 0.0)*100:.2f}% |

### Metric AÃ§Ä±klamalarÄ±
- **Accuracy**: TÃ¼m tahminlerin ne kadarÄ±nÄ±n doÄŸru olduÄŸu
- **Precision**: Positive tahminlerin ne kadarÄ±nÄ±n gerÃ§ekten positive olduÄŸu
- **Recall**: GerÃ§ek positive'lerin ne kadarÄ±nÄ±n bulunduÄŸu
- **F1-Score**: Precision ve Recall'Ä±n harmonik ortalamasÄ±
- **ROC-AUC**: ROC eÄŸrisi altÄ±ndaki alan (model ayrÄ±ÅŸtÄ±rma gÃ¼cÃ¼)
- **PR-AUC**: Precision-Recall eÄŸrisi altÄ±ndaki alan (dengesiz veri iÃ§in)

## 4. Confusion Matrix

"""
    
    if cm is not None:
        report_content += f"""
| | Predicted Negative | Predicted Positive |
|---|---|---|
| **Actual Negative** | {cm[0,0]} (TN) | {cm[0,1]} (FP) |
| **Actual Positive** | {cm[1,0]} (FN) | {cm[1,1]} (TP) |

- **True Negative (TN)**: {cm[0,0]} - DoÄŸru negatif tahminler
- **False Positive (FP)**: {cm[0,1]} - YanlÄ±ÅŸlÄ±kla positive denilen negatifler
- **False Negative (FN)**: {cm[1,0]} - KaÃ§Ä±rÄ±lan positive'ler
- **True Positive (TP)**: {cm[1,1]} - DoÄŸru positive tahminler

"""
    
    # GÃ¶reli path'ler (output_dir'e gÃ¶re)
    report_content += f"""
![Confusion Matrix](evaluation_results/confusion_matrix.png)

## 5. ROC Curve
ROC (Receiver Operating Characteristic) eÄŸrisi, modelin farklÄ± eÅŸik deÄŸerlerindeki performansÄ±nÄ± gÃ¶sterir.
AUC (Area Under Curve) deÄŸeri ne kadar yÃ¼ksekse model o kadar iyidir (maksimum 1.0).

![ROC Curve](evaluation_results/roc_curve.png)

## 6. Precision-Recall Curve
Precision-Recall eÄŸrisi, Ã¶zellikle dengesiz veri setlerinde modelin performansÄ±nÄ± deÄŸerlendirmek iÃ§in kullanÄ±lÄ±r.

![PR Curve](evaluation_results/precision_recall_curve.png)

## 7. Error Analysis

### Toplam Hatalar
- **Toplam yanlÄ±ÅŸ tahmin**: {len(errors_df)}
"""
    
    if len(errors_df) > 0:
        fp_count = len(errors_df[errors_df['error_type'] == 'False Positive'])
        fn_count = len(errors_df[errors_df['error_type'] == 'False Negative'])
        report_content += f"""- **False Positive**: {fp_count} (negatif ama positive dendi)
- **False Negative**: {fn_count} (positive ama negatif dendi)
"""
    
    report_content += f"""
### En HatalÄ± {min(15, len(errors_df))} Tahmin
(GÃ¼vene gÃ¶re sÄ±ralÄ± - yÃ¼ksek gÃ¼venle yapÄ±lan hatalar)

{error_list_str}

DetaylÄ± hata analizi iÃ§in: `evaluation_results/error_analysis.csv`

## 8. Performance Summary

### GÃ¼Ã§lÃ¼ YÃ¶nler
"""
    
    # GÃ¼Ã§lÃ¼ yÃ¶nleri otomatik belirle
    if metrics['accuracy'] >= 0.95:
        report_content += "- âœ… Ã‡ok yÃ¼ksek doÄŸruluk oranÄ± (>95%)\n"
    elif metrics['accuracy'] >= 0.90:
        report_content += "- âœ… YÃ¼ksek doÄŸruluk oranÄ± (>90%)\n"
    
    if metrics['precision'] >= 0.95:
        report_content += "- âœ… Ã‡ok dÃ¼ÅŸÃ¼k False Positive oranÄ±\n"
    
    if metrics['recall'] >= 0.95:
        report_content += "- âœ… Ã‡ok dÃ¼ÅŸÃ¼k False Negative oranÄ± (neredeyse tÃ¼m transitler bulunuyor)\n"
    
    if metrics.get('roc_auc', 0) >= 0.95:
        report_content += "- âœ… MÃ¼kemmel sÄ±nÄ±f ayrÄ±ÅŸtÄ±rma gÃ¼cÃ¼ (ROC-AUC >0.95)\n"
    
    report_content += """
### Ä°yileÅŸtirme AlanlarÄ±
"""
    
    # Ä°yileÅŸtirme alanlarÄ±
    if metrics['recall'] < 0.90:
        report_content += "- âš ï¸ Recall dÃ¼ÅŸÃ¼k - bazÄ± transitler kaÃ§Ä±rÄ±lÄ±yor\n"
    
    if metrics['precision'] < 0.90:
        report_content += "- âš ï¸ Precision dÃ¼ÅŸÃ¼k - fazla False Positive var\n"
    
    if len(errors_df) > 0 and errors_df['confidence'].mean() > 0.7:
        report_content += "- âš ï¸ Hatalar yÃ¼ksek gÃ¼venle yapÄ±lÄ±yor - model aÅŸÄ±rÄ± gÃ¼venli\n"
    
    if metrics['accuracy'] < 0.90:
        report_content += "- âš ï¸ Genel doÄŸruluk artÄ±rÄ±labilir\n"
    
    report_content += """
## 9. Next Steps & Recommendations

### Model Ä°yileÅŸtirme
- [ ] Daha fazla epoch ile eÄŸitim dene
- [ ] FarklÄ± augmentation stratejileri uygula
- [ ] HatalÄ± Ã¶rnekleri manuel incele
- [ ] Ensemble modeller dene (birden fazla model kombinasyonu)

### Veri Ä°yileÅŸtirme
- [ ] Daha fazla Ã¶rnek topla (Ã¶zellikle hata yapÄ±lan sÄ±nÄ±flar iÃ§in)
- [ ] Veri kalitesini artÄ±r (daha iyi grafik Ã¼retimi)
- [ ] FarklÄ± grafik tiplerini dene (farklÄ± normalizasyon, binning)

### DeÄŸerlendirme
- [ ] FarklÄ± mission'lar iÃ§in ayrÄ± performans analizi yap
- [ ] FarklÄ± yÄ±ldÄ±z tÃ¼rleri iÃ§in performansÄ± incele
- [ ] Production ortamÄ±nda gerÃ§ek verilerle test et

---

**Rapor OluÅŸturma Tarihi**: {current_date}  
**Script**: `scripts/evaluate_model.py`  
**Otomatik Ã¼retildi** âœ¨
"""
    
    # GÃ¼ncellenen raporu kaydet
    output_report_path = Path(output_dir) / 'MODEL_PERFORMANCE_REPORT.md'
    with open(output_report_path, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    print(f"  Kaydedildi: {output_report_path}")
    
    # Ana klasÃ¶re de kopyala
    main_report_path = Path('MODEL_PERFORMANCE_REPORT.md')
    with open(main_report_path, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    print(f"  Ana rapor gÃ¼ncellendi: {main_report_path}")


def print_final_summary(metrics, cm):
    """
    Final Ã¶zeti yazdÄ±r.
    
    Args:
        metrics: Metrikler
        cm: Confusion matrix
    """
    print("\n" + "=" * 60)
    print("DEÄERLENDÄ°RME Ã–ZETÄ°")
    print("=" * 60)
    
    print(f"\nSÄ±nÄ±flandÄ±rma Metrikleri:")
    print(f"  Accuracy:  {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)")
    print(f"  Precision: {metrics['precision']:.4f}")
    print(f"  Recall:    {metrics['recall']:.4f}")
    print(f"  F1-Score:  {metrics['f1_score']:.4f}")
    
    if 'roc_auc' in metrics and metrics['roc_auc'] > 0:
        print(f"\nAUC Metrikleri:")
        print(f"  ROC-AUC: {metrics['roc_auc']:.4f}")
        print(f"  PR-AUC:  {metrics['pr_auc']:.4f}")
    
    if cm is not None:
        print(f"\nConfusion Matrix:")
        print(f"  True Negative:  {cm[0,0]:4d}")
        print(f"  False Positive: {cm[0,1]:4d}")
        print(f"  False Negative: {cm[1,0]:4d}")
        print(f"  True Positive:  {cm[1,1]:4d}")
        
        total = cm.sum()
        print(f"\n  Toplam: {total}")
        print(f"  DoÄŸru tahmin: {cm[0,0] + cm[1,1]} ({(cm[0,0] + cm[1,1])/total*100:.2f}%)")
        print(f"  YanlÄ±ÅŸ tahmin: {cm[0,1] + cm[1,0]} ({(cm[0,1] + cm[1,0])/total*100:.2f}%)")
    
    print("=" * 60)


def main():
    """Ana fonksiyon"""
    parser = argparse.ArgumentParser(
        description='Model performans deÄŸerlendirmesi',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument('--predictions', '--pred_csv', dest='predictions', required=True,
                       help='Tahmin CSV dosyasÄ± (Ã¶rn: predictions_detail.csv)')
    parser.add_argument('--output_dir', '--out_dir', dest='output_dir', default='evaluation_results',
                       help='Ã‡Ä±ktÄ± klasÃ¶rÃ¼')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("MODEL PERFORMANS DEÄERLENDÄ°RMESÄ°")
    print("=" * 60)
    print(f"Tahmin dosyasÄ±: {args.predictions}")
    print(f"Ã‡Ä±ktÄ± klasÃ¶rÃ¼: {args.output_dir}")
    print("=" * 60)
    
    # 1. Tahminleri yÃ¼kle
    df = load_predictions(args.predictions)
    if df is None:
        sys.exit(1)
    
    # 2. True label kontrolÃ¼
    has_labels, df = check_true_labels(df)
    
    if not has_labels:
        print("\nâš  True label bulunamadÄ± - metrik hesaplamasÄ± yapÄ±lamaz!")
        print("Sadece tahmin istatistikleri:")
        pred_dist = df['pred_label'].value_counts()
        print(f"  Positive tahmin: {pred_dist.get(1, 0)}")
        print(f"  Negative tahmin: {pred_dist.get(0, 0)}")
        sys.exit(0)
    
    # 3. Labeled verileri filtrele
    df_labeled = df[df['true_label'].notna()].copy()
    df_labeled['true_label'] = df_labeled['true_label'].astype(int)
    
    print(f"\nâœ“ {len(df_labeled)} etiketli gÃ¶rsel ile deÄŸerlendirme yapÄ±lacak")
    
    # 4. Arrays hazÄ±rla
    y_true = df_labeled['true_label'].values
    y_pred = df_labeled['pred_label'].values
    y_proba_pos = df_labeled['prob_positive'].values
    
    # 5. Metrikleri hesapla
    metrics = calculate_metrics(y_true, y_pred, y_proba_pos)
    
    # 6. Confusion matrix
    cm = create_confusion_matrix(y_true, y_pred, args.output_dir)
    
    # 7. ROC curve
    create_roc_curve(y_true, y_proba_pos, args.output_dir)
    
    # 8. PR curve
    create_pr_curve(y_true, y_proba_pos, args.output_dir)
    
    # 9. Hata analizi
    errors_df = analyze_errors(df_labeled, args.output_dir)
    
    # 10. Metrik Ã¶zetini kaydet
    save_metrics_summary(metrics, cm, args.output_dir)
    
    # 11. Performans raporunu gÃ¼ncelle
    update_performance_report(metrics, cm, errors_df, args.output_dir)
    
    # 12. Final Ã¶zet
    print_final_summary(metrics, cm)
    
    print("\n" + "=" * 60)
    print("âœ“ DEÄERLENDÄ°RME TAMAMLANDI")
    print("=" * 60)
    print(f"SonuÃ§lar: {Path(args.output_dir).resolve()}")
    print(f"Performans raporu: MODEL_PERFORMANCE_REPORT.md")
    print("=" * 60)


if __name__ == "__main__":
    main()

